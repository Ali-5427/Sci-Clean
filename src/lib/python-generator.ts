import type { ProcessedCsvData, ConfirmedTypes, DataType } from './types';

function getPandasTypeConversion(column: string, type: DataType): string {
    const safeCol = column.includes("'") ? `"${column}"` : `'${column}'`;
    switch (type) {
        case 'NUMERIC':
            return `df[${safeCol}] = pd.to_numeric(df[${safeCol}], errors='coerce')`;
        case 'TEXT':
            return `df[${safeCol}] = df[${safeCol}].astype(str).replace('nan', np.nan)`;
        case 'DATE':
            // This is a generic date parser. For production, more robust format detection would be needed.
            return `df[${safeCol}] = pd.to_datetime(df[${safeCol}], errors='coerce')`;
        case 'CATEGORICAL':
            return `df[${safeCol}] = df[${safeCol}].astype('category')`;
        default:
            return `# No conversion rule for type: ${type}`;
    }
}

export function generatePythonScript(
    processedData: ProcessedCsvData,
    confirmedTypes: ConfirmedTypes
): string {
    const now = new Date();
    const typeConversionSteps = Object.entries(confirmedTypes)
        .map(([columnName, { type }]) => {
            const conversionCode = getPandasTypeConversion(columnName, type);
            return `
# Convert '${columnName}' to ${type}
print("Converting '${columnName}' to ${type}...")
${conversionCode}
`;
        })
        .join('');

    return `"""
CLEANED DATA GENERATION SCRIPT
Generated by Sci-Clean Studio
Date: ${now.toISOString()}
Original File: ${processedData.fileName}
Original Hash (SHA-256): ${processedData.fileHash}

This script replicates the exact cleaning steps performed in Sci-Clean Studio.
Run this script to regenerate the cleaned data from the original file.
"""

import pandas as pd
import numpy as np

# --- CONFIGURATION ---
INPUT_FILE = '${processedData.fileName}'
OUTPUT_FILE = 'cleaned_${processedData.fileName}'

# --- LOAD DATA ---
try:
    print(f"Loading data from {INPUT_FILE}...")
    df = pd.read_csv(INPUT_FILE)
    print(f"Loaded: {len(df)} rows × {len(df.columns)} columns")
except FileNotFoundError:
    print(f"ERROR: Input file not found at '{INPUT_FILE}'. Please ensure the script is in the same directory as the data file.")
    exit()

# --- STEP 1: DATA TYPE CONVERSIONS ---
print("\\n--- Step 1: Converting data types ---")
${typeConversionSteps}
print("Data type conversion complete.")

# --- STEP 2: HANDLE MISSING VALUES (IMPUTATION) ---
print("\\n--- Step 2: Handling missing values ---")
missing_before = df.isnull().sum().sum()
print(f"Total missing values before imputation: {missing_before}")

# The imputation strategy depends heavily on the nature of your data.
# Common strategies are provided below. Uncomment the one that best fits your needs for each column.
# Note: Imputation is done on a per-column basis.

# --- Example Imputation Strategies ---
# Choose the column you want to impute, for example 'age':
# target_column_to_impute = 'age'

# Strategy 1: Mean Imputation (for numeric data)
# Good for normally distributed data. Can be skewed by outliers.
# if target_column_to_impute in df.columns and df[target_column_to_impute].isnull().sum() > 0:
#     print(f"Imputing missing '{target_column_to_impute}' values with the mean...")
#     mean_value = df[target_column_to_impute].mean()
#     df[target_column_to_impute].fillna(mean_value, inplace=True)

# Strategy 2: Median Imputation (for numeric data)
# More robust to outliers than the mean.
# if target_column_to_impute in df.columns and df[target_column_to_impute].isnull().sum() > 0:
#     print(f"Imputing missing '{target_column_to_impute}' values with the median...")
#     median_value = df[target_column_to_impute].median()
#     df[target_column_to_impute].fillna(median_value, inplace=True)

# Strategy 3: Mode Imputation (for categorical data)
# Good for filling in the most common category.
# if target_column_to_impute in df.columns and df[target_column_to_impute].isnull().sum() > 0:
#     print(f"Imputing missing '{target_column_to_impute}' values with the mode...")
#     mode_value = df[target_column_to_impute].mode()[0]
#     df[target_column_to_impute].fillna(mode_value, inplace=True)

# Strategy 4: Forward-Fill (ffill) Imputation (for time-series or ordered data)
# Propagates the last valid observation forward.
# if target_column_to_impute in df.columns and df[target_column_to_impute].isnull().sum() > 0:
#     print(f"Imputing missing '{target_column_to_impute}' values with forward-fill...")
#     df[target_column_to_impute].fillna(method='ffill', inplace=True)

# After imputation, you might still have missing values at the beginning of the file.
# You can fill these with a specific value, like 0.
# df.fillna(0, inplace=True)

final_missing_values_after_imputation = df.isnull().sum().sum()
if missing_before > 0:
    imputed_count = missing_before - final_missing_values_after_imputation
    if imputed_count > 0:
        print(f"Imputed {imputed_count} missing values.")
print(f"Total missing values after imputation step: {final_missing_values_after_imputation}")


# --- STEP 3: SAVE CLEANED DATA ---
print("\\n--- Step 3: Saving cleaned data ---")
df.to_csv(OUTPUT_FILE, index=False)
print(f"✅ Cleaned data saved to: {OUTPUT_FILE}")

# --- FINAL SUMMARY ---
print("\\n--- Summary ---")
print(f"Final shape: {len(df)} rows × {len(df.columns)} columns")
final_missing_values = df.isnull().sum().sum()
print(f"Total missing values in cleaned file: {final_missing_values} cells")
print("Script finished.")
`;
}
